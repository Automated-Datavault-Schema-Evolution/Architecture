@startuml
actor "File System" as FS

FS -> [Filewatcher] : CSV file change detected
[Filewatcher] -> [Kafka Service] : Sends change event
[Kafka Service] -> [Kafka Consumer / PySpark] : Streams data
[Kafka Consumer / PySpark] -> [Datalake Handler] : Sends dataframe

note right of [Datalake Handler]
  Configurable Modes:
  - Real-time streaming & updates
  - Batch-changes (using Kafka streaming)
end note

[Datalake Handler] -> [Postgres/Parquet Storage] : Stores data (config dependent)
[Postgres/Parquet Storage] -> [Data Vault] : Ingest data
[Data Vault] -> [Presentation Layer] : Provide star schema view
@enduml
